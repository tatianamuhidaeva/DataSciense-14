# DataSciense-14
## Module 0
## Module 1
Цель: прорешать задачи, изучить pandas
## Module 2
Цель: подготовить dataset для дальнейшей обработки

- Проведите первичную обработку данных. Так как данных много, стоит написать функции, которые можно применять к столбцам определённого типа.
- Посмотрите на распределение признака для числовых переменных, устраните выбросы.
- Оцените количество уникальных значений для номинативных переменных.
- По необходимости преобразуйте данные
- Проведите корреляционный анализ количественных переменных
- Отберите не коррелирующие переменные.
- Проанализируйте номинативные переменные и устраните те, которые не влияют на предсказываемую величину (в нашем случае — на переменную score).
- Не забудьте сформулировать выводы относительно качества данных и тех переменных, которые вы будете использовать в дальнейшем построении модели.

**Отзыв ментора:**

Здравствуйте, Татьяна!

Спасибо за вашу замечательную работу! Честно говоря, у меня нет совершенно критических замечаний, ниже я приведу свои имеющиеся комментарии по критериям.

Критерий 1:

Jupyter Notebook - очень мощный инструмент для создания в том числе интерактивных отчетов и презентаций. И нужно всегда иметь в виду, что если вы делаете подобный отчёт не для себя, то как и любую презентацию его желательно делать понятным, информативным и структурированным. И в этом деле никак не обойтись без текста. Здорово, что вы делаете текстовые вставки с помощью Markdown режима. Тем не менее, в некоторых местах рассуждения и описания шагов вы сделали в формате комментариев к коду ( т.е. с символом #). В такой форме текстовую информацию может быть неудобно воспринимать.

Повторюсь, если вы просто делаете ваши личные рабочие записи, то в принципе вы и так знаете, зачем вы делаете тот или иной ход (хотя и тут может быть полезно оставлять текстовые комментарии, они позволят вам лучше структурировать ваши личные эксперименты. Не зря же учёные порой ведут дневники об экспериментах - чтобы потом в них не запутаться). Если вы отправляете вашу интерактивную презентацию другому человеку, не важно, ментору или коллеге - нужно постараться, чтобы этот человек понял ваш ход мысли. Совет: пишите больше текстовых комментариев, описывающих ваши действия и результаты, но не переусердствуйте: ищите гармоничный баланс, да поможет вам бритва Оккама. :)

Про поиск пустых строк и выбросов (Критерий 2):

Нельзя просто так взять и ... заменить отсутствующие значения модами, медианами или чем-либо ещё!
Так мы получаем много дополнительных одинаковых значений, которые не факт, что соответствуют действительности. Заполнение пропусков очень сложная и тонкая тема, и там применяются весьма сложные техники. В нашей таблице не так уж и велико отношение количества пропусков к количеству всех элементов (меньше 10%), можно было бы просто перевести все пропуски в None и остановиться на этом. Также непонятно решение заменить пропуски значениями 'others' в нескольких колонках с номинативными переменными. Помните, в EDA нет обязательной цели заполнить АБСОЛЮТНО ВСЕ пропуски в данных. Идея с заполнением колонок Medu и Fedu на основе взаимной корреляции кажется неплохой, но... Тут есть большая ошибка: переведя str в int , вы как будто предположили, что ... < 'services' < 'health' < 'teacher' - иными словами, что тут присутствует ранжирование, что одно как-то лучше другого. Эти две переменные не являются качественными ранговыми, формально у вас нет права применять корреляционный анализ в такой ситуации. О типах переменных я ещё напишу в Критерие 4.
Заполнение пропусков довольно тонкая тема, и там применяются сложные техники. Дополнительную информацию можно подчерпнуть например тут:

http://www.machinelearning.ru/wiki/images/9/90/Kayumov_course_work_imputer_methods.pdf

https://ru.coursera.org/lecture/unsupervised-learning/obrabotka-propuskov-3mRlZ

На всякий случай ещё скажу про следующее: удаляя всю запись из-за одного, казалось бы, ненормального или отсутствующего значения, вы теряете много информации о значениях других параметрах в этой записи. Нам они могут быть очень важны. С другой стороны, удалить записи, где целевая переменная (в нашем случае - score) не имеет значений (NaN) - можно, так как такие записи нам ничем не помогут: отследить корреляции или применить статистический тест для качественных переменных мы не сможем, а любое заполнение в любом случае будет вносить лишнюю ошибку, так как это не будет истинно точным значением.

Выбросы в колонках famrel и Fedu действительно, скорее всего, были опечатками. Вы сделали абсолютно верно, что скорректировали их.
Так же и с переменной absences: скорее всего, значение 385 было опечаткой, остальные же вполне себе могут быть правдой. Например, ученики находятся на домашнем обучении, etc.

Заполнение пропусков довольно тонкая тема, и там применяются сложные техники. Про заполнение пропусков можно почитать тут:

http://www.machinelearning.ru/wiki/images/9/90/Kayumov_course_work_imputer_methods.pdf

https://ru.coursera.org/lecture/unsupervised-learning/obrabotka-propuskov-3mRlZ

Про гистограммы и выводы (Критерий 3):

В целом, тут вопросов нет, однако кажется, что вы построили гистограммы не для всех переменных, для которых можно было. Вместо этого при рассмотрении некоторых переменных вы выводили аблицу, сколько раз встречается каждое её значение. Визуальной информативности в этом мало, а скроллинга экрана - много. Проще было бы построить гистограмму)

Про корреляционный анализ и анализ номинативных переменных (Критерий 4):

Вы сделали правильный глобальный ход: рассматривать численные и номинативные (качественные) переменные по отдельности. И очень правильно отделили их друг от друга, действительно: качественных переменных в данном датасете больше, чем кажется. Они просто закодированы числами. И к этим переменным можно применять статистический тест.

Почитайте ещё вот эту статью. В конце (важно!) в том числе затрагивается то, о чем я упомянул в Критерие 2, что переводить номинативные качественные переменные в численные для проведения корреляционного анализа - некорректно.
https://towardsdatascience.com/variable-types-and-examples-cf436acaf769

Про корреляционный анализ:

1. Как я уже писал выше, ваш отчет должен быть предельно понятен для других читателей. Графики должны быть репрезентативны и должны хорошо отражать рассматриваемые зависимости. Непонятно, о какой репрезентативности может идти речь, когда вы используете функцию sns.heatmap() к довольно большой таблице. Делайте лаконичнее и информативнее: если вы сразу видите, что тот или иной результат плохо визуально представим - просто не добавляйте его.

2. Вы абсолютно правильно определили, что столбец 'studytime, granular' может быть удален из таблицы, так как является дублирующим.

Про анализ номинативных переменных:

Боксплоты для номинативных переменных построены хорошо и репрезентативно.

Очень-очень хорошо, что вы вникли в суть функции get_stat_dif() и внесли в неё свой взгляд.

Дополнительная информация:
Статистический тест может спокойно работать и на данных, где есть NaN и None в рассматриваемых столбцах (именно поэтому мы и могли оставить None в датасете) - надо использовать флаг nan_policy = 'omit' (почитать подробно: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html).

Что можно добавить: почитайте про t-test Уэлча. По-хорошему, именно его нужно использовать в данной задаче, так как мы не уверены, что дисперсии у сравниваемых распределений одинаковы.
https://en.wikipedia.org/wiki/Welch%27s_t-test
Поддержка этого критерия встроена в функцию ttest_ind, и активируется путём передачи параметра equal_var = False, вместо True, которое идёт по умолчанию.

Про выводы (Критерий 5):

Грамотное краткое представление результатов в выводе очень важно, так как помогает структурировать всё, что вы сделали. Очень здорово, что вы сформулировали основные “вехи” вашей работы и, самое главное, определили список параметров, которые вы решили использовать. Пожеланием тут было бы делать вывод всё-таки чуть более подробным: рассказывать о важных операциях с данными, неочевидных характеристиках датасета, которые вы нашли, о результатах анализов переменных, etc.

Тем не менее, даже учитывая небольшие недочёты, все равно округляю оценку в LMS до 3/3, потому что работа действительно выполнена качественно. Желаю вам дальнейших успехов и не опускать планку!

С уважением,

Глеб Карпов, ментор SkillFactory


## module 4. SQL

**Задача**
Напомним, что вам предстоит выяснить, от каких самых малоприбыльных рейсов из Анапы мы можем отказаться в зимнее время. Вы не знаете, по каким критериям ваше руководство будет отбирать рейсы, поэтому решаете собрать как можно больше информации, содержащейся в вашей базе, в один датасет. 

Исходя из того, что прибыльность рейса — это разница между доходом от продаж билетов и расходом на полет, соберите такой датасет, который позволит оценить эти цифры. 

Самая простая модель оценки прибыльности: стоимость билетов - стоимость топлива на рейс (для оценки последнего вам необходим километраж рейса или длительность полета), но вы можете предложить другую модель.

Ваш датасет должен обязательно включать id рейса и города вылета (Анапа) и прилета. Он также  может включать такие данные, как модель самолёта и его характеристики, суммарную стоимость всех билетов на рейсе, затраченное время в полёте и прочее — на ваше усмотрение.

**Отзыв ментора:**

Представлен достаточно подробный анализ. Основным критерием убыточности можно считать расходы на топливо более 15% от суммы продажи билетов. Необходимо учитывать и заполняемость салона самолёта. Рейсы с заполняемостью менее 75% можно считать убыточными. В данном работе указано, что убыточными можно считать 29 рейсов. Всё-таки рекомендуется пересмотреть критерий прибыльности.


Здравствуйте, Татьяна!
Результаты проекта проверены. Признаю, что вами подготовлена очень
хорошая и аккуратная работа. Особенно меня поразил итоговый запрос. Он
позволяет выгрузить огромное количество полезной информации, при этом,
скорость обработки данных очень высокая. Обсудим достоинства и
недостатки полученных результатов.
Решения заданий из 4 и 5 раздела. Дело в том, что представленные вами
решения на github достаточно трудно проверять. По непонятной причине
символы в строках “слиплись”. Скажем так, что здесь я просто просмотрел
код. Его работоспособность проверить не удалось.
Как уже отмечалось ранее, подготовлен очень добротный итоговый запрос.
Особенность его заключается в том, что он позволяет выгрузить много нужной
информации. Понятно, что достаточно хороший запрос позволяет выгрузить
полезный датасет. Сам запрос читается легко, а там, где возникают трудности,
помогают комментарии. Можно сказать, что данная часть работы удалась вам
на твёрдую пятёрку!
Очень важно, что в работе представлен анализ данных в блокноте jn. Не все
стараются выполнить дополнительное задание, а зря. Здесь есть уникальная
возможность ещё раз попрактиковать работу с библиотекой pandas языка
python. Единственное, в чём можно поспорить с вами в данной части проекта,
так это по поводу выбора критерия убыточности, который, в итоге, позволил
отнести 29 рейсов к убыточным. Было бы наилучшим вариантом, если вы
указали ссылку на ресурс, где сказано, что расходы на рейс составляют 60%
от максимального дохода рейса. В противном случае всё выглядит не совсем
убедительно. В остальном всё смотрится достаточно хорошо.
Презентация результатов работы. Данная часть раскрыта достаточно хорошо.
Многие пункты подробно описаны и имеют понятные комментарии. Моё
мнение, что не хватает графиков, которые вы получили на этапе анализа. Про
выводы было сказано ранее. Всё-таки считаю, что критерий убыточности
нужно пересмотреть.
Если возникнут вопросы по отзыву, вы можете обратиться ко мне в Slack [@
Сергей Добдин] в канале, соответствующем модулю с заданием/проектом
[#0_project_4-работа-с-базой-данных].
Подведём итоги работы. Работа выполнена и заслуживает высокой оценки!
Отлично подготовлен итоговый запрос, что свидетельствует о хорошем
знании базовых операторов языка SQL. Татьяна, хочется поблагодарить вас за
аккуратность выполненного проекта. Очень приятно проверять и оценивать
подобные работы. Порядок очень сильное качество! Старайтесь
придерживаться его и в будущем. Удачи в дальнейшем обучении!
Отзыв подготовил ментор @ Сергей Добдин

Буду ждать вас на итоговый созвон-вебинар по текущему проекту в пятницу в
20:00. На нём мы разберём основные ошибки группы и посмотрим эталонное
решение. До встречи на вебинаре!
Ссылка на вебинар –
https://us02web.zoom.us/j/85341744801?pwd=RFlidzRaeWUrVFNLRzJ0YUxCd

## module_5

**Описание**
Добро пожаловать на хакатон по машинному обучению!

Представьте, что вы работаете стажером в отделении регионального банка. Вы все также делаете запросы к базам данных и строите отчеты. Вы поймали себя на мысли, что представляли работу дата-саентиста совсем иначе…

И вот сегодня, когда вы уже были на пороге отчаяния, ваш начальник пришел к вам с долгожданной новостью. Будем строить модель!

“Отлично,” – думаете вы, – “наконец-то смогу заняться настоящей работой!”

Вашей задачей будет построить скоринг модель для вторичных клиентов банка, которая бы предсказывала вероятность дефолта клиента. Для этого нужно будет определить значимые параметры заемщика.

**Ответ**

Внутри ноутбуков отличное вступление!
Помните, что иногда объединение может сыграть злую шутку при операция с масштабированием, нормализацией и тд

Можно подглядеть в pandas profiling, можно вывести кол-во уникальных значений по признаку, но окончательное решение всегда выносить по описанию датасета. Например, признаков регион если бы был такой. Регионов 84шт, но это не числовой признак.

Первый вывод верный, второе решение - ошибка

Смотреть на корреляцию с целевой переменной как минимум бесполезно. И при наличии такой мало информации о признаках, что есть home address и work_address абсолютно разные признаки. И нормально видеть повышенную корреляцию, тк обычно люди живут рядом с местом работы. Не стоит удалять это признак + 0,7 не очень большое значение.

Добавьте визуализацию числовой переменной и отметьте, что классы явно не сбалансированы и это может стать причиной потери денег. И как видно на матрицы мы практически не распознаем дефолтных клиентов = выдаем всем.
Здесь также нужны выводы
И нужно проверить не переобучение, скорее всего оно есть.

Класс получился отличным

Но я бы разбила метод по метрикам на несколько и отделила бы визуализацию get_f1()  plot_f1()  get_roc_auc() plot_roc_auc и тд
Вам ведь не всегда нужны все метрики. Так ваш класс станет универсальным. + не стоит называть его LogReg так как его можно применить для любого алгоритма классификации. Просто можно Class Classifier 

Плюсом будет использование сводным табличек по всем проведенным экспериментам. И оформление экспериментов + добавление выводов по процессу обучения.
Обоснование выбора лучшей модели, сравнение и тд тоже стоит добавить.
Здорово, что удалось попробовать командную работу. Вы хорошо справились с заданием! 
Качество кода (соблюдение стандартов оформления pep-8, комментирование кода, наличие выводов). Оформление проекта на Github — 2 балла.
Качество разведывательного анализа данных (визуализация, очистка данных, работа с выбросами) — 3 балла.
Работа с признаками (выбор признаков, генерация новых признаков) — 2 балла.
Качество решения: Результат метрики ROC AUC — 3 балла.
Качество решения: обоснованность выводов и общая логика решения — 2 балла.

Отзыв подготовила: Анна Нохрина, ментор проекта.
Если возникнут вопросы по отзыву, вы можете обратиться ко мне в канал 
#0_project_4-компьютер_говорит_нет в Slack. Постараюсь ответить на ваши вопросы и разобраться с моментами, которые вызывают трудности. Удачи в обучении!

https://drive.google.com/file/d/1leQYTo2tUXQeb5Dhc1RS06yHXZON1dhy/view?usp=sharing

## module_6

Карьерный модуль

## module_7. Выбираем авто выгодно

**Описание:**
Представьте, что вы работаете в компании, которая занимается продажей автомобилей с пробегом. Основная задача компании и ее менеджеров - максимально быстро находить выгодные предложения (проще говоря, купить ниже рынка, а продать дороже рынка).

Вам поставлена задача создать модель, которая будет предсказывать стоимость автомобиля по его характеристикам.
Если наша модель работает хорошо, то мы сможем быстро выявлять выгодные предложения (когда желаемая цена продавца ниже предсказанной рыночной цены). Это значительно ускорит работу менеджеров и повысит прибыль компании.

**А где данные для обучения модели?**
Так исторически сложилось, что компания изначально не собирала данные. Есть только небольшой датасет с историей продаж, которого для обучения модели будет явно мало. Его мы будем использовать для теста (В ЛидерБорде), остальное придется собрать самим… Вспоминаем модуль по парсингу сайтов или ищем готовые датасеты.

**Ответ ментора**
https://drive.google.com/file/d/1j9CTTrW9ftNapdk8TYFtePT_3DS2m8uJ/view

## module_8

- Computer Vision
- Neural Languages
- Machine Learning
